My Initial model had an input that took 32 filters, had 1 convolution and pooling faze with the pool size of (2, 2), as well as a hidden layer of 128 filters, and a dropout of 50%, ensuring an even dispersal of productivity. The output layer uses softmax as its activation

Initially, I ran the program without normalizing the pixel values, this caused my accuracy to be sub 6%, and my loss to be over 3.0. After dividing by 255, converting the values to be in the range of 0 and 1, my accuracy skyrocketed to 96.7% and my loss plummeted to 0.13.

Surprisingly, increasing the input filter from 8 all the way to 32 only increases the accuracy by 0.5%. Having doubled the convolution and pooling phase, and changing the first pool size to (4, 4), dropped the accuracy by a substantial amount, leaving it at 76% and the loss raised to 0.8. Not only that, but doubling the hidden layers and halving both the filters drops the accuracy to 80%. However, only halving the filter on the hidden layer only renders the accuracy to 92%. And Finaly, increasing the dropout to 0.7 from 0.5 dropped my accuracy by 2% and changed my loss from 0.15 to 0.28.